We sincerely thank all reviewers for their constructive comments.
R1Q1, R4Q3: Thank you for your suggestions. While this work primarily focused on feed forward architectures, in the future we plan to exploit unstructured sparsity in Transformers and recurrent networks that are now supported via FINN-T. 
R1Q2, R4Q6: The maximum achievable frequency is one of the results of our DSE, which are captured in Tabes I-IV that compares the speedup achieved across Fold, Unfold, and Unfold+Pruning cases. We will further clarify the setup and analysis of these ablation experiments in the final manuscript.
R1Q3, R3Q1–6: Thank you for pointing this out, we will include the per-layer sparsity details in the final manuscript.
R1Q4: The text refers to a fully unfolded sparse layer that requires fewer resources compared to its dense folded version. We will revise text in final manuscript.
R3Q2: We currently use magnitude pruning as the initialisation strategy, as it has been shown in [41] to be closest to a lottery ticket winning. On the quantise strategies supported, any Brevitas strategy can be supported through our flow. 
R3Q5: The specific figure reflects the trend of validation accuracy during fine-tuning, as well as the single-run average performance on the test set. We will update the figure caption to provide the details.
R4Q1, R4Q8: Thank you for pointing this out, we will fix the text in the final manuscript. The sparse mask tuning approach from [41] is integrated in a hardware-aware manner within our framework to accelerate accuracy recovery in case of drop-offs. While [41] provides detailed ablation studies on floating-point models (speed and accuracy advantages), the core perspective of this work is the utilisation paradigm of unstructured sparsity for mixed precision networks in a hardware-efficient manner. 
R4Q2, R4Q6, R4Q7: The “T” column in Table III captures the overheads. Our method incurs only 3% of the design time to achieve performance close to a fully unfolded network. With our co-design approach, high-performance fully unfolded networks, which would otherwise be infeasible on resource constrained FPGAs, can be successfully and efficiently deployed through the resource savings achieved by exploiting unstructured sparsity. We will reword the table descriptions for better readability.
R4Q4: The baseline core that we compare against in the paper reports synthesis results, and hence our comparisons used the same approach for fairness. We have results from implementation and on-device measurements which will be reported in the final paper.
R4Q5: The main focus of this work is on exploring unstructured sparsity in mixed precision networks for efficient hardware implementation in resource constrained devices. The performance of this approach is compared against other works using the same DNNs for fairness. Ablation studies in Tables I–IV capture the effectiveness of our framework. State-of-the-art GPUs only support fixed precision 2:4 structured sparsity, which is different to the mixed-precision unstructured sparsity that can be best exploited using custom datapaths on FPGAs, and enabling this design flow is the key contribution of this work. 