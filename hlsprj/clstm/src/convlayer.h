/******************************************************************************
 *  Copyright (c) 2019, Xilinx, Inc.
 *  All rights reserved.
 *
 *  Redistribution and use in source and binary forms, with or without
 *  modification, are permitted provided that the following conditions are met:
 *
 *  1.  Redistributions of source code must retain the above copyright notice,
 *     this list of conditions and the following disclaimer.
 *
 *  2.  Redistributions in binary form must reproduce the above copyright
 *      notice, this list of conditions and the following disclaimer in the
 *      documentation and/or other materials provided with the distribution.
 *
 *  3.  Neither the name of the copyright holder nor the names of its
 *      contributors may be used to endorse or promote products derived from
 *      this software without specific prior written permission.
 *
 *  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
 *  AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,
 *  THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 *  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR
 *  CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
 *  EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 *  PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;
 *  OR BUSINESS INTERRUPTION). HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
 *  WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR
 *  OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF
 *  ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 *
 *****************************************************************************/

/******************************************************************************
 *
 *  Authors: Giulio Gambardella <giuliog@xilinx.com>
 *           Thomas B. Preusser <thomas.preusser@utexas.edu>
 *             Marie-Curie Fellow, Xilinx Ireland, Grant Agreement No. 751339
 *           Christoph Doehring <cdoehrin@xilinx.com>
 *           Timoteo Garcia Bertoa <timoteog@xilinx.com>  
 *
 *  \file convlayer.h
 *
 *  Library of templated HLS functions for BNN deployment.
 *  This file lists a set of convenience functions used to implement
 *  convolutional layers.
 *
 *****************************************************************************/

#ifndef CONVLAYER_H
#define CONVLAYER_H

#include <ap_int.h>
#include <hls_stream.h>

#include "streamtools.h"
#include "slidingwindow.h"
#include "mvau.hpp"
#include "tmrcheck.hpp"

/**
 * \brief 	Convolutional layer implementation
 *
 * The function implements a generic convolutional layer, and it's basically composed of the sliding window generator
 * implemeting the im2col algorithm and the Matrix_Vector_Activate_Batch function to perform computation.
 * 
 * \tparam ConvKernelDim 	Dimension of the convolutional kernel (assumed square)
 * \tparam IFMChannels 		Number of Input Feature Maps
 * \tparam IFMDim 			Width and Height of the Input Feature Map (assumed square)
 * \tparam OFMChannels 		Number of Output Feature Maps
 * \tparam OFMDim 			Width and Height of the Output Feature Map (assumed square)
 * \tparam SIMD 			Number of input columns computed in parallel
 * \tparam PE 				Number of output rows computed in parallel
 * \tparam TSrcI 			DataType of the input activation (as used in the MAC)
 * \tparam TDstI 			DataType of the output activation (as generated by the activation)
 * \tparam TWeightI 		DataType of the weights (as used in the MAC)
 * \tparam InStreamW 		Width of the input stream
 * \tparam OutStreamW 		Width of the output stream
 * \tparam TW 				DataType of the weights matrix - safely deducible from the paramaters
 * \tparam TA 				DataType of the activation class (e.g. thresholds) - safely deducible from the paramaters
 * \tparam R 				DataType for the resource used for FPGA implementation of the MAC  - safely deducible from the paramaters
 *
 * \param in 				Input stream
 * \param out 				Output stream
 * \param weights 			Weights matrix (currently supports BinaryWeights or FixedPointWeights)
 * \param activation 		Activation class
 * \param reps 				Number of time the function has to be repeatedly executed (e.g. number of images)
 * \param r 				Resource type for the hardware implementation of the MAC block
 */

template<
		unsigned int ConvKernelDim,
		unsigned int IFMChannels,
		unsigned int IFMDim,
		unsigned int OFMChannels,
		unsigned int OFMDim,

		unsigned int SIMD, 				// number of SIMD lanes
		unsigned int PE,				// number of PEs

		typename TSrcI,      // redefine I/O interpretation as needed for input activations
		typename TDstI,		// redefine I/O interpretation as needed for output activations
		typename TWeightI,	// redefine I/O interpretation as needed for weigths

		int InStreamW, int OutStreamW,  // safely deducible (stream width must be int though!)
		typename TW,   typename TA,  typename R
>
void ConvLayer_Batch(hls::stream<ap_uint<InStreamW>>  &in,
			    hls::stream<ap_uint<OutStreamW>> &out,
			    TW const        &weights,
			    TA const        &activation,
			    unsigned const   reps,
				R const &r) {
#pragma HLS INLINE
  unsigned const MatrixW = ConvKernelDim * ConvKernelDim * IFMChannels;
  unsigned const MatrixH = OFMChannels;
  unsigned const InpPerImage = IFMDim * IFMDim * IFMChannels * TSrcI::width / InStreamW;
  hls::stream<ap_uint<SIMD*TSrcI::width> > wa_in("StreamingConvLayer_Batch.wa_in");
  hls::stream<ap_uint<SIMD*TSrcI::width> > convInp("StreamingConvLayer_Batch.convInp");
  hls::stream<ap_uint<PE*TDstI::width> > mvOut("StreamingConvLayer_Batch.mvOut");
  StreamingDataWidthConverter_Batch<InStreamW, SIMD*TSrcI::width, InpPerImage>(in, wa_in, reps);
  ConvolutionInputGenerator<ConvKernelDim, IFMChannels, TSrcI::width, IFMDim,
			OFMDim, SIMD,1>(wa_in, convInp, reps, ap_resource_dflt());
  Matrix_Vector_Activate_Batch<MatrixW, MatrixH, SIMD, PE, 1, TSrcI, TDstI, TWeightI>
    (static_cast<hls::stream<ap_uint<SIMD*TSrcI::width>>&>(convInp),
     static_cast<hls::stream<ap_uint<PE*TDstI::width>>&>  (mvOut),
     weights, activation, reps* OFMDim * OFMDim, r);
  StreamingDataWidthConverter_Batch<PE*TDstI::width, OutStreamW, OFMDim * OFMDim * (OFMChannels / PE)>(mvOut, out, reps);

}
template<
    unsigned int ConvKernelDim,     // 卷积核尺寸
    unsigned int IFMChannels,       // 输入通道数
    unsigned int IFMDim,           // 输入特征图尺寸
    unsigned int OFMChannels,      // 输出通道数
    unsigned int OFMDim,           // 输出特征图尺寸

    unsigned int SIMD,             // SIMD通道数
    unsigned int PE,               // PE数量

    typename TSrcI,     // 输入激活数据类型转换
    typename TDstI,     // 输出激活数据类型转换
    typename TWeightI,  // 权重数据类型转换

    int InStreamW, int OutStreamW, // 输入输出流位宽
    typename TW, typename TA, typename R
>
class ConvLayer_Batch_new {
private:
    // 内部缓冲区
    TW m_weights[OFMChannels][ConvKernelDim * ConvKernelDim * IFMChannels];
    TA m_activation_params[OFMChannels];

public:
    // 初始化方法
    void init(__attribute__((unused)) unsigned const ofm,
              __attribute__((unused)) unsigned const pe) const {
        #pragma HLS inline
        // 初始化逻辑
    }

public:
    // 主执行函数
    void execute(hls::stream<ap_uint<InStreamW>> &in,
                 hls::stream<ap_uint<OutStreamW>> &out,
                 unsigned const reps,
                 R const &r,
				 TA  const &activation) {
        #pragma HLS INLINE

        unsigned const MatrixW = ConvKernelDim * ConvKernelDim * IFMChannels;
        unsigned const MatrixH = OFMChannels;
        unsigned const InpPerImage = IFMDim * IFMDim * IFMChannels * TSrcI::width / InStreamW;

        hls::stream<ap_uint<SIMD*TSrcI::width>> wa_in("conv_layer_wa_in");
        hls::stream<ap_uint<SIMD*TSrcI::width>> convInp("conv_layer_convInp");
        hls::stream<ap_uint<PE*TDstI::width>> mvOut("conv_layer_mvOut");

        // 数据宽度转换
        StreamingDataWidthConverter_Batch<InStreamW, SIMD*TSrcI::width, InpPerImage>
            (in, wa_in, reps);

        // 卷积输入生成
        ConvolutionInputGenerator<ConvKernelDim, IFMChannels, TSrcI::width, IFMDim,
                                OFMDim, SIMD, 1>
            (wa_in, convInp, reps, ap_resource_dflt());

        // 矩阵向量激活计算
        Matrix_Vector_Activate_Batch<MatrixW, MatrixH, SIMD, PE, 1, TSrcI, TDstI, TWeightI>
            (static_cast<hls::stream<ap_uint<SIMD*TSrcI::width>>&>(convInp),
             static_cast<hls::stream<ap_uint<PE*TDstI::width>>&>(mvOut),
             m_weights, activation, reps * OFMDim * OFMDim, ap_resource_dflt());

        // 输出数据宽度转换
        StreamingDataWidthConverter_Batch<PE*TDstI::width, OutStreamW,
                                        OFMDim * OFMDim * (OFMChannels / PE)>
            (mvOut, out, reps);
    }

public:
    // 方法1：从一维数组加载权重
    void load_weights_from_array(const TW weights[OFMChannels * ConvKernelDim * ConvKernelDim * IFMChannels]) {
        #pragma HLS inline
        for (unsigned ofm = 0; ofm < OFMChannels; ofm++) {
            for (unsigned idx = 0; idx < ConvKernelDim * ConvKernelDim * IFMChannels; idx++) {
                #pragma HLS unroll
                unsigned global_idx = ofm * ConvKernelDim * ConvKernelDim * IFMChannels + idx;
                m_weights[ofm][idx] = weights[global_idx];
            }
        }
    }

    // 方法2：从二维数组加载权重
    void load_weights_from_2darray(const TW weights[OFMChannels][ConvKernelDim * ConvKernelDim * IFMChannels]) {
        #pragma HLS inline
        for (unsigned ofm = 0; ofm < OFMChannels; ofm++) {
            for (unsigned idx = 0; idx < ConvKernelDim * ConvKernelDim * IFMChannels; idx++) {
                #pragma HLS unroll
                m_weights[ofm][idx] = weights[ofm][idx];
            }
        }
    }

    // 方法3：从hls::stream加载权重
    template<typename T>
    void load_weights_from_stream(hls::stream<T>& weight_stream) {
        #pragma HLS inline
        for (unsigned ofm = 0; ofm < OFMChannels; ofm++) {
            for (unsigned idx = 0; idx < ConvKernelDim * ConvKernelDim * IFMChannels; idx++) {
                #pragma HLS pipeline
                T data = weight_stream.read();
                m_weights[ofm][idx] = static_cast<TW>(data);
            }
        }
    }

    // 方法4：设置单个权重值
    void set_weight(unsigned ofm, unsigned kernel_row, unsigned kernel_col, unsigned ifm, TW value) {
        #pragma HLS inline
        if (ofm < OFMChannels && kernel_row < ConvKernelDim &&
            kernel_col < ConvKernelDim && ifm < IFMChannels) {
            unsigned idx = (kernel_row * ConvKernelDim * IFMChannels) +
                          (kernel_col * IFMChannels) + ifm;
            m_weights[ofm][idx] = value;
        }
    }

    // 方法5：常量初始化权重
    void init_weights_constant(TW value) {
        #pragma HLS inline
        for (unsigned ofm = 0; ofm < OFMChannels; ofm++) {
            for (unsigned idx = 0; idx < ConvKernelDim * ConvKernelDim * IFMChannels; idx++) {
                #pragma HLS unroll
                m_weights[ofm][idx] = value;
            }
        }
    }

    template<unsigned int IFM, unsigned int KW>
    void load_weights_from_4darray_generic(const TW weights[OFMChannels][IFM][ConvKernelDim][KW]) {
        #pragma HLS inline
        static_assert(IFM == IFMChannels, "IFM dimension must match IFMChannels");

        for (unsigned ofm = 0; ofm < OFMChannels; ofm++) {
            for (unsigned ifm = 0; ifm < IFMChannels; ifm++) {
                for (unsigned kh = 0; kh < ConvKernelDim; kh++) {
                    for (unsigned kw = 0; kw < KW; kw++) {
                        #pragma HLS unroll
                        unsigned internal_idx = kh * (ConvKernelDim * IFMChannels) +
                                               kw * IFMChannels + ifm;
                        m_weights[ofm][internal_idx] = weights[ofm][ifm][kh][kw];
                    }
                    // 如果KW小于ConvKernelDim，填充剩余位置为0
                    for (unsigned kw = KW; kw < ConvKernelDim; kw++) {
                        #pragma HLS unroll
                        unsigned internal_idx = kh * (ConvKernelDim * IFMChannels) +
                                               kw * IFMChannels + ifm;
                        m_weights[ofm][internal_idx] = TW(0);
                    }
                }
            }
        }
    }


    // 激活参数设置方法
    void set_activation_param(unsigned ofm, TA value) {
        #pragma HLS inline
        if (ofm < OFMChannels) {
            m_activation_params[ofm] = value;
        }
    }

    // 批量设置激活参数
    void load_activation_params(const TA params[OFMChannels]) {
        #pragma HLS inline
        for (unsigned ofm = 0; ofm < OFMChannels; ofm++) {
            #pragma HLS unroll
            m_activation_params[ofm] = params[ofm];
        }
    }
};

//// 保持向后兼容的函数接口
//template<
//    unsigned int ConvKernelDim,
//    unsigned int IFMChannels,
//    unsigned int IFMDim,
//    unsigned int OFMChannels,
//    unsigned int OFMDim,
//    unsigned int SIMD,
//    unsigned int PE,
//    typename TSrcI = Identity,
//    typename TDstI = Identity,
//    typename TWeightI = Identity,
//    int InStreamW, int OutStreamW,
//    typename TW, typename TA, typename R
//>
//void ConvLayer_Batch(hls::stream<ap_uint<InStreamW>> &in,
//                     hls::stream<ap_uint<OutStreamW>> &out,
//                     TW const &weights,
//                     TA const &activation,
//                     unsigned const reps,
//                     R const &r) {
//    #pragma HLS INLINE
//
//    // 创建临时对象并执行
//    static ConvLayer_Batch<ConvKernelDim, IFMChannels, IFMDim, OFMChannels, OFMDim,
//                          SIMD, PE, TSrcI, TDstI, TWeightI, InStreamW, OutStreamW, TW, TA, R>
//        conv_layer;
//
//    // 这里需要根据weights和activation参数来设置内部状态
//    // 注意：这需要根据具体的权重和激活参数类型来调整
//
//    conv_layer.execute(in, out, reps, r);
//}

/**
 * \brief   Convolutional layer implementation with STMR
 *
 * The function implements a generic convolutional layer, and it's basically composed of the sliding window generator
 * implemeting the im2col algorithm and the Matrix_Vector_Activate_Batch function to perform computation. Additionally,
 * a TMR checker function performs error checks and outputs valid data.
 *
 * \tparam ConvKernelDim    Dimension of the convolutional kernel (assumed square)
 * \tparam IFMChannels      Number of Input Feature Maps
 * \tparam IFMDim           Width and Height of the Input Feature Map (assumed square)
 * \tparam OFMChannels      Number of Output Feature Maps
 * \tparam OFMDim           Width and Height of the Output Feature Map (assumed square)
 * \tparam SIMD             Number of input columns computed in parallel
 * \tparam PE               Number of output rows computed in parallel
 * \tparam NUM_RED          Number of redundancies (or triplicated channels)
 * \tparam REDF             Redundancy factor (3 to triplicate)
 * \tparam MAX_CH_WIDTH     Value to determine the precision of channel indexes
 * \tparam TSrcI            DataType of the input activation (as used in the MAC)
 * \tparam TDstI            DataType of the output activation (as generated by the activation)
 * \tparam TWeightI         DataType of the weights (as used in the MAC)
 * \tparam InStreamW        Width of the input stream
 * \tparam OutStreamW       Width of the output stream
 * \tparam TW               DataType of the weights matrix - safely deducible from the paramaters
 * \tparam TA               DataType of the activation class (e.g. thresholds) - safely deducible from the paramaters
 * \tparam R                DataType for the resource used for FPGA implementation of the MAC  - safely deducible from the paramaters
 *
 * \param in                Input stream
 * \param out               Output stream
 * \param weights           Weights matrix (currently supports BinaryWeights or FixedPointWeights)
 * \param activation        Activation class
 * \param reps              Number of time the function has to be repeatedly executed (e.g. number of images)
 * \param r                 Resource type for the hardware implementation of the MAC block
 * \param errortype         Flag to inform redundancy check results. 0 if no faults, 1 if one PE is faulty, 2 if all differ
 * \param channel_mask      Value with binary channel masks (1 if channel is triplicated, 0 otherwise)
 * \param red_ch_index      Array of redundant triplets' indexes. Each position stores the first triplicated channel index of a triplet
 */

template<
        unsigned int ConvKernelDim,
        unsigned int IFMChannels,
        unsigned int IFMDim,
        unsigned int OFMChannels,
        unsigned int OFMDim,

        unsigned int SIMD,              // number of SIMD lanes
        unsigned int PE,                // number of PEs

        unsigned int NUM_RED,           // number of redundant channels
        unsigned int REDF,              // redundancy factor (3 to triplicate)
        unsigned int MAX_CH_WIDTH,      // width to represent channel indexes

        typename TSrcI = Identity,      // redefine I/O interpretation as needed for input activations
        typename TDstI = Identity,      // redefine I/O interpretation as needed for output activations
        typename TWeightI = Identity,   // redefine I/O interpretation as needed for weigths

        int InStreamW, int OutStreamW,  // safely deducible (stream width must be int though!)
        typename TW,   typename TA,  typename R
>
void ConvLayer_Batch_TMR(hls::stream<ap_uint<InStreamW>>  &in,
                         hls::stream<ap_uint<OutStreamW>> &out,
                         TW const        &weights,
                         TA const        &activation,
                         unsigned const   reps,
                         R const &r,
                         ap_uint<2> &errortype,
                         ap_uint<OFMChannels> channel_mask,
                         ap_uint<MAX_CH_WIDTH> red_cha_index[NUM_RED]) {
#pragma HLS INLINE
  unsigned const MatrixW = ConvKernelDim * ConvKernelDim * IFMChannels;
  unsigned const MatrixH = OFMChannels;
  unsigned const InpPerImage = IFMDim*IFMDim;

  hls::stream<ap_uint<SIMD*TSrcI::width> > wa_in("StreamingConvLayer_Batch.wa_in");
  hls::stream<ap_uint<SIMD*TSrcI::width> > convInp("StreamingConvLayer_Batch.convInp");
  hls::stream<ap_uint<PE*TDstI::width> > mvOut("StreamingConvLayer_Batch.mvOut");
  hls::stream<ap_uint<OFMChannels*TDstI::width> > tmr_in("StreamingConvLayer_Batch.tmr_in");

  StreamingDataWidthConverter_Batch<InStreamW, SIMD*TSrcI::width, InpPerImage>(in, wa_in, reps);

  //Sliding window unit
  ConvolutionInputGenerator<ConvKernelDim, IFMChannels, TSrcI::width, IFMDim,
            OFMDim, SIMD,1>(wa_in, convInp, reps, ap_resource_dflt());

  //MVTU
  Matrix_Vector_Activate_Batch<MatrixW, MatrixH, SIMD, PE, 1, TSrcI, TDstI, TWeightI>
    (static_cast<hls::stream<ap_uint<SIMD*TSrcI::width>>&>(convInp),
     static_cast<hls::stream<ap_uint<PE*TDstI::width>>&>  (mvOut),
     weights, activation, reps* OFMDim * OFMDim, r);

  StreamingDataWidthConverter_Batch<PE*TDstI::width, OFMChannels*TDstI::width, OFMDim * OFMDim * (OFMChannels / PE)>(mvOut, tmr_in, reps);

  //Error check
  TMRCheck_Batch<TDstI::width, OFMChannels, NUM_RED, REDF, OFMDim, MAX_CH_WIDTH>(tmr_in, out, errortype, channel_mask, red_cha_index, reps);
}

/**
 * \brief 	Convolutional layer implementation
 *
 * The function implements a generic convolutional layer, and it's basically composed of the sliding window generator
 * implemeting the im2col algorithm and the Matrix_Vector_Activate_Batch function to perform computation.
 *
 * \tparam ConvKernelDim 	Dimension of the convolutional kernel (assumed square)
 * \tparam IFMChannels 		Number of Input Feature Maps
 * \tparam IFMDim 			Width and Height of the Input Feature Map (assumed square)
 * \tparam OFMChannels 		Number of Output Feature Maps
 * \tparam OFMDim 			Width and Height of the Output Feature Map (assumed square)
 * \tparam STRIDE 			Stride of the convolutional kernel
 *
 * \tparam SIMD 			Number of input columns computed in parallel
 * \tparam PE 				Number of output rows computed in parallel
 * \tparam MMV 				Number of output pixels computed in parallel
 *
 * \tparam TSrcI 			DataType of the input activation (as used in the MAC)
 * \tparam TDstI 			DataType of the output activation (as generated by the activation)
 * \tparam TWeightI 		DataType of the weights (as used in the MAC)
 * \tparam InStreamW 		Width of the input stream
 * \tparam OutStreamW 		Width of the output stream
 * \tparam TW 				DataType of the weights matrix - safely deducible from the paramaters
 * \tparam TA 				DataType of the activation class (e.g. thresholds) - safely deducible from the paramaters
 * \tparam R 				DataType for the resource used for FPGA implementation of the MAC  - safely deducible from the paramaters
 *
 * \param in 				Input stream
 * \param out 				Output stream
 * \param weights 			Weights matrix (currently supports BinaryWeights or FixedPointWeights)
 * \param activation 		Activation class
 * \param reps 				Number of time the function has to be repeatedly executed (e.g. number of images)
 * \param r 				Resource type for the hardware implementation of the MAC block
 */
template<
		unsigned int ConvKernelDim,
		unsigned int IFMChannels,
		unsigned int IFMDim,
		unsigned int OFMChannels,
		unsigned int OFMDim,
		unsigned int STRIDE,

		unsigned int SIMD,				// number of SIMD lanes
		unsigned int PE,				// number of PEs
		unsigned int MMV,

		typename TSrcI = Identity,      // redefine I/O interpretation as needed for input activations
		typename TDstI = Identity,		// redefine I/O interpretation as needed for output activations
		typename TWeightI = Identity,	// redefine I/O interpretation as needed for weigths

		int InStreamW, int OutStreamW,  // safely deducible (stream width must be int though!)
		typename TW,   typename TA,  typename R
>
void ConvLayer_Batch_MMV(hls::stream<ap_uint<InStreamW>>  &in,
			    hls::stream<ap_uint<OutStreamW>> &out,
			    TW const        &weights,
			    TA const        &activation,
			    unsigned const   reps,
				R const &r) {
#pragma HLS INLINE
  unsigned const MatrixW = ConvKernelDim * ConvKernelDim * IFMChannels;
  unsigned const MatrixH = OFMChannels;
  unsigned const InpPerImage = IFMDim*IFMDim*IFMChannels * TSrcI::width/InStreamW;
  const unsigned int mmvReps = (reps * OFMDim * OFMDim) / MMV;

  hls::stream<ap_uint<SIMD * TSrcI::width> > wa_in("StreamingConvLayerMMV_Batch.wa_in");
  hls::stream<MultiChanData<MMV, SIMD *TSrcI::width> > convInp("StreamingConvLayerMMV_Batch.convInp");
  hls::stream<MultiChanData<MMV, PE * TDstI::width> > mmv2dwc("StreamingConvLayerMMV_Batch.mmv2dwc");
  hls::stream<MultiChanData<MMV, OFMChannels * TDstI::width>> dwc2flat("dwc2flat");
  hls::stream<ap_uint<MMV * OFMChannels * TDstI::width> > mvOut("StreamingConvLayerMMV_Batch.mvOut");

  StreamingDataWidthConverter_Batch<InStreamW, SIMD * TSrcI::width, InpPerImage>(in, wa_in, reps);

  ConvolutionInputGenerator_MMV<ConvKernelDim, IFMChannels, TSrcI::width, IFMDim,
			OFMDim, SIMD, STRIDE, MMV>(wa_in, convInp, reps, ap_resource_lutram());
  Matrix_Vector_Activate_Batch<MatrixW, MatrixH, SIMD, PE, MMV, TSrcI, TDstI, TWeightI>
    (static_cast<hls::stream<MultiChanData<MMV,SIMD*TSrcI::width>>&>(convInp),
     static_cast<hls::stream<MultiChanData<MMV,PE*TDstI::width>>&>(mmv2dwc),
     weights, activation, mmvReps, r);
  
  MultiChanDataWidthConverter_Batch<PE * TDstI::width, OFMChannels * TDstI::width, OFMDim * OFMDim * (OFMChannels / PE), MMV>(mmv2dwc, dwc2flat, reps);
  FlattenMultiChanData<MMV, OFMChannels * TDstI::width>(dwc2flat, mvOut, mmvReps);
  StreamingDataWidthConverter_Batch<MMV * OFMChannels * TDstI::width, OutStreamW, OFMDim * OFMDim / MMV>(mvOut, out, reps);
  
}

//
//// self defined conv function class
//template<
//    unsigned int ConvKernelDim,
//    unsigned int IFMChannels,
//    unsigned int IFMDim,
//    unsigned int OFMChannels,
//    unsigned int OFMDim,
//
//    unsigned int SIMD,                 // number of SIMD lanes
//    unsigned int PE,                // number of PEs
//
//    typename TSrcI = Identity,      // redefine I/O interpretation as needed for input activations
//    typename TDstI = Identity,        // redefine I/O interpretation as needed for output activations
//    typename TWeightI = Identity,    // redefine I/O interpretation as needed for weigths
//
//    int InStreamW = 32, int OutStreamW = 32,  // 提供默认值
//    typename TW = int,              // 添加默认类型
//    typename TA = ThresholdActivation<int>,  // 添加默认类型
//    typename R = int                // 添加默认类型
//>
//class myConvLayer_Batch {
//public:
//    // Constructor - empty, just like ThresholdActivation
//    myConvLayer_Batch() {
//#pragma HLS inline
//    }
//
//public:
//    // Forward method - mirrors the original function signature
//    void forward(hls::stream<ap_uint<InStreamW>>& in,
//                 hls::stream<ap_uint<OutStreamW>>& out,
//                 TW const& weights,
//                 TA const& activation,
//                 unsigned const reps,
//                 R const& r) const {
//#pragma HLS INLINE
//
//        unsigned const MatrixW = ConvKernelDim * ConvKernelDim * IFMChannels;
//        unsigned const MatrixH = OFMChannels;
//        unsigned const InpPerImage = IFMDim * IFMDim * IFMChannels * TSrcI::width / InStreamW;
//
//        hls::stream<ap_uint<SIMD*TSrcI::width>> wa_in("StreamingConvLayer_Batch.wa_in");
//        hls::stream<ap_uint<SIMD*TSrcI::width>> convInp("StreamingConvLayer_Batch.convInp");
//        hls::stream<ap_uint<PE*TDstI::width>> mvOut("StreamingConvLayer_Batch.mvOut");
//
//        StreamingDataWidthConverter_Batch<InStreamW, SIMD*TSrcI::width, InpPerImage>(in, wa_in, reps);
//        ConvolutionInputGenerator<ConvKernelDim, IFMChannels, TSrcI::width, IFMDim, OFMDim, SIMD, 1>
//            (wa_in, convInp, reps, ap_resource_dflt());
//        Matrix_Vector_Activate_Batch<MatrixW, MatrixH, SIMD, PE, 1, TSrcI, TDstI, TWeightI>
//            (static_cast<hls::stream<ap_uint<SIMD*TSrcI::width>>&>(convInp),
//             static_cast<hls::stream<ap_uint<PE*TDstI::width>>&>(mvOut),
//             weights, activation, reps * OFMDim * OFMDim, r);
//        StreamingDataWidthConverter_Batch<PE*TDstI::width, OutStreamW, OFMDim * OFMDim * (OFMChannels / PE)>
//            (mvOut, out, reps);
//    }
//};
//
//
//
//// MyConvLayer conv_layer;
//
//
//// auto weights = load_weights();
//// ThresholdActivation<int> activation(128);
//// ResourceConfig r_config = get_resource_config();
//
//
//// conv_layer.forward(input_stream, output_stream, weights, activation, 10, r_config);

#endif
